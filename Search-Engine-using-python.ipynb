{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1o33OlH9fqqn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/menilik/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/menilik/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fohbxePKgAdp",
        "outputId": "bc5c93a9-ed16-4100-cb95-e310edccf89b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     0         1         2         3         4    5\n",
            "0  0.0  0.085345  0.000000  0.000000  0.233406  0.0\n",
            "1  0.0  0.000000  0.000000  0.000000  0.142318  0.0\n",
            "2  0.0  0.085345  0.000000  0.000000  0.116703  0.0\n",
            "3  0.0  0.000000  0.000000  0.173941  0.000000  0.0\n",
            "4  0.0  0.000000  0.000000  0.173941  0.000000  0.0\n",
            "5  0.0  0.000000  0.167583  0.000000  0.000000  0.0\n",
            "6  0.0  0.000000  0.167583  0.000000  0.000000  0.0\n",
            "7  0.0  0.085345  0.137421  0.000000  0.000000  0.0\n",
            "8  0.0  0.000000  0.167583  0.000000  0.000000  0.0\n",
            "9  0.0  0.104077  0.000000  0.000000  0.000000  0.0\n",
            "(228, 6)\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "stopwords_list = stopwords.words('english')\n",
        "\n",
        "english_stopset = set(stopwords.words('english')).union(\n",
        "                  {\"things\", \"that's\", \"something\", \"take\", \"don't\", \"may\", \"want\", \"you're\",\n",
        "                   \"set\", \"might\", \"says\", \"including\", \"lot\", \"much\", \"said\", \"know\",\n",
        "                   \"good\", \"step\", \"often\", \"going\", \"thing\", \"things\", \"think\",\n",
        "                   \"back\", \"actually\", \"better\", \"look\", \"find\", \"right\", \"example\",\n",
        "                                                                  \"verb\", \"verbs\"})\n",
        "\n",
        "##docs = retrieve_docs_and_clean()\n",
        "docs = ['i loved you ethiopia, stored elements in Compress find Sparse Ethiopia is the greatest country in the world of nation at universe',\n",
        "        \n",
        "        'also, sometimes, the same words can have multiple different ‘lemma’s. So, based on the context it’s used, you should identify the \\\n",
        "        part-of-speech (POS) tag for the word in that specific context and extract the appropriate lemma. Examples of implementing this comes \\\n",
        "        in the following sections countries.ethiopia With a planned.The name that the Blue Nile river loved took in Ethiopia is derived from the \\\n",
        "        Geez word for great to imply its being the river of rivers The word Abay still exists in ethiopia major languages',\n",
        "        \n",
        "        'With more than  million people, ethiopia is the second most populous nation in Africa after Nigeria, and the fastest growing \\\n",
        "         economy in the region. However, it is also one of the poorest, with a per capita income',\n",
        "        \n",
        "        'The primary purpose of the dam ethiopia is electricity production to relieve Ethiopia’s acute energy shortage and for electricity export to neighboring\\\n",
        "         countries.ethiopia With a planned.',\n",
        "        \n",
        "        'The name that the Blue Nile river loved takes in Ethiopia \"abay\" is derived from the Geez blue loved word for great to imply its being the river of rivers The \\\n",
        "         word Abay still exists in Ethiopia major languages to refer to anything or anyone considered to be superior.',\n",
        "        \n",
        "        'Two non-upgraded loved turbine-generators with MW each are the first loveto go into operation with loved MW delivered to the national power grid. This early power\\\n",
        "         generation will start well before the completion']\n",
        "\n",
        "title = ['Two upgraded', 'Loved Turbine-Generators', 'Operation With Loved', 'National', 'Power Grid', 'Generator']\n",
        "\n",
        "keywords = ['two','non','loved','ethiopia','operation','grid','power','fight','survive']  #we can generate keywords from articls using 'spacy'\n",
        "\n",
        "\n",
        "documents_clean = []\n",
        "documents_cleant = []\n",
        "for d in docs:\n",
        "    document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)  #Replace non-ASCII characters with space\n",
        "    document_test = re.sub(r'@\\w+', '', document_test)  #eliminate duplicate whitespaces/ # Remove Mentions\n",
        "    document_test = document_test.lower() #converting to lower\n",
        "    document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test) #cleaning punctuation\n",
        "    document_test = re.sub(r'[0-9]', '', document_test) #replacing number with empity string\n",
        "    document_test = re.sub(r'\\s{2,}', ' ', document_test)  # Remove the doubled space\n",
        "    documents_clean.append(document_test)\n",
        "    documents_cleant.append(document_test)\n",
        "\n",
        "\n",
        "#Lemmatization the words      #better than https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
        "lemmer=WordNetLemmatizer()\n",
        "new_docs=[' '.join([lemmer.lemmatize(docs) for docs in text.split(',')]) for text in docs]  #Lemmatization the words/description\n",
        "titles = [' '.join([lemmer.lemmatize(title).strip() for title in text.split(' ')]) for text in title]   #Lemmatization the title\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='word',\n",
        "                              ngram_range=(1, 2),\n",
        "                              min_df=0.002,\n",
        "                              max_df=0.99,\n",
        "                              max_features=10000,\n",
        "                              lowercase=True,\n",
        "                              stop_words=english_stopset)\n",
        "\n",
        "X = vectorizer.fit_transform(new_docs)\n",
        "\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(X.T.toarray())\n",
        "print(df.head(10))\n",
        "print(df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEytLYCrg_ca",
        "outputId": "e062060d-5ba4-41e0-bf89-9cbacf126be5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Searching. Full Result: \n",
            "\n",
            "searched items :  ethiopia\n",
            "Article with the Highest Cosine Similarity Values: \n",
            "Similaritas score:  0.2673433484640173\n",
            "National\n",
            "The primary purpose of the dam ethiopia is electricity production to relieve Ethiopia’s acute energy shortage and for electricity export to neighboring         countries.ethiopia With a planned.\n",
            "\n",
            "\n",
            "Similaritas score:  0.2232032354875907\n",
            "Two upgraded\n",
            "i loved you ethiopia  stored elements in Compress find Sparse Ethiopia is the greatest country in the world of nation at universe\n",
            "\n",
            "\n",
            "Similaritas score:  0.15996489348662396\n",
            "Loved Turbine-Generators\n",
            "also  sometimes  the same words can have multiple different ‘lemma’s. So  based on the context it’s used  you should identify the         part-of-speech (POS) tag for the word in that specific context and extract the appropriate lemma. Examples of implementing this comes         in the following sections countries.ethiopia With a planned.The name that the Blue Nile river loved took in Ethiopia is derived from the         Geez word for great to imply its being the river of rivers The word Abay still exists in ethiopia major languages\n",
            "\n",
            "\n",
            "Similaritas score:  0.14582664099950898\n",
            "Power Grid\n",
            "The name that the Blue Nile river loved takes in Ethiopia \"abay\" is derived from the Geez blue loved word for great to imply its being the river of rivers The          word Abay still exists in Ethiopia major languages to refer to anything or anyone considered to be superior.\n",
            "\n",
            "\n",
            "Similaritas score:  0.08585732144317441\n",
            "Operation With Loved\n",
            "With more than  million people  ethiopia is the second most populous nation in Africa after Nigeria  and the fastest growing          economy in the region. However  it is also one of the poorest  with a per capita income\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "def get_similar_articles(q,t, df):\n",
        "  print(\"Done Searching. Full Result: \\n\")\n",
        "  print(\"searched items : \", q)\n",
        "  print(\"Article with the Highest Cosine Similarity Values: \")\n",
        "  search_rank ={}\n",
        "  top_results=10\n",
        "  q = [q]\n",
        "  t = [t]\n",
        "\n",
        "  q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
        "  q_vect = vectorizer.transform(t).toarray().reshape(df.shape[0],)\n",
        "  sim = {}\n",
        "  titl = {}\n",
        "\n",
        "  for i in range(len(new_docs)) and range(len(titles)):                                            \n",
        "    sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)  #Calculate the similarity\n",
        "    # Or we can use cosine)similarity library both are the same\n",
        "    titl[i] = np.dot(df.loc[:, i].values, q_vect) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vect)\n",
        "\n",
        "  sim_sorted = sorted(sim.items(),key=lambda x : x[1], reverse=True)[:min(len(sim), top_results)]\n",
        "  sim_sortedt = sorted(titl.items(),key=lambda x : x[1], reverse=True)[:min(len(titl), top_results)]\n",
        "\n",
        "\n",
        "  for i, v in sim_sorted and sim_sortedt:    # Print the articles and their similarity values\n",
        "    if v != 0.0:\n",
        "      print(\"Similaritas score: \", v)\n",
        "      zip(titles, new_docs)\n",
        "      print(titles[i])\n",
        "      print(new_docs[i])\n",
        "      print('\\n')\n",
        "\n",
        "\n",
        "lemma_ops = 'electrical productions'\n",
        "#q1 = 'electrical productions'\n",
        "list1 = nltk.word_tokenize(lemma_ops)\n",
        "q1 = ' '.join([lemmer.lemmatize(lemma_ops) for lemma_ops in list1])\n",
        "\n",
        "get_similar_articles(q1,q1, df)\n",
        "print('-'*100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "addis search with function.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
